{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2ee3532-5f25-41ab-940f-b01b4a73a36c",
   "metadata": {},
   "source": [
    "# Lab: Web Scraping and Data Extraction with Python\n",
    "\n",
    "You are tasked with building a web scraper to extract structured data from the Wikipedia page for **\"Samsung.\"** (`https://en.wikipedia.org/wiki/Samsung`). Follow the steps below to complete the task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947d5def-3a16-48a8-bf76-41c3ace78e3d",
   "metadata": {},
   "source": [
    "### 1. Import Relevant Libraries\n",
    "Import all the necessary libraries for web scraping and data manipulation:\n",
    "\n",
    "- `requests` for making HTTP requests.\n",
    "- `BeautifulSoup` from `bs4` for parsing HTML.\n",
    "- `pandas` for tabular data manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca03ffba-1d1e-485a-b788-10e872f72130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94cb333-8c6c-48e1-b4dd-c2f52f3dd8d6",
   "metadata": {},
   "source": [
    "### 2. Perform HTTP Request\n",
    "- Send an HTTP GET request to the URL: `https://en.wikipedia.org/wiki/Samsung`.\n",
    "- Save the response object for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dea8355-f37f-41ea-af8e-b85a86c70949",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/Samsung\"\n",
    "response = requests.get(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918e6412-30f6-4776-b25a-78dab85510fb",
   "metadata": {},
   "source": [
    "### 3. Check the Request Status\n",
    "- Ensure the HTTP request is successful by checking the status code of the response.\n",
    "- Print a message:\n",
    "  - **Success:** If the status code is `200`.\n",
    "  - **Error:** If any other status code is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a425fb1-1151-4d0a-81c7-d097574b4306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request successful!\n"
     ]
    }
   ],
   "source": [
    "if response.status_code == 200:\n",
    "    print(\"Success\")\n",
    "else:\n",
    "    print(\"Error:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc6364e-961e-4a2d-a6ae-c08baad7fff6",
   "metadata": {},
   "source": [
    "### 4. Build the Extraction Model\n",
    "- Parse the HTML content using `BeautifulSoup`.\n",
    "- Use the `\"html.parser\"` as the parser.\n",
    "- Save the parsed object for further extraction tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c58020-2db2-4e6e-9609-999df211fd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, \"html.parser\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1711066-c9b2-40f5-b29e-ce692cf4b4b1",
   "metadata": {},
   "source": [
    "### 5. Extract Headings\n",
    "- Use `BeautifulSoup` to extract all headings (`<h1>`, `<h2>`, `<h3>`).\n",
    "- Save the extracted text into a structured format, such as a Python dictionary or a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547d6f00-43b8-4227-b06b-2da930c75a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "headings = {\n",
    "    \"h1\": [h.text.strip() for h in soup.find_all(\"h1\")],\n",
    "    \"h2\": [h.text.strip() for h in soup.find_all(\"h2\")],\n",
    "    \"h3\": [h.text.strip() for h in soup.find_all(\"h3\")]\n",
    "}\n",
    "\n",
    "headings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8830748-6612-4a0f-bef6-c73ca3c95604",
   "metadata": {},
   "source": [
    "### 6. Extract All Paragraphs\n",
    "- Extract all the text content within `<p>` tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7ea132-f7a4-4eb1-b9ca-281ed841939a",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = [p.text.strip() for p in soup.find_all(\"p\")]\n",
    "\n",
    "paragraphs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d426c8-556b-417f-bfa1-dced69662dcc",
   "metadata": {},
   "source": [
    "### 7. Extract All Links\n",
    "- Extract all hyperlinks (links within `<a>` tags).\n",
    "- Collect:\n",
    "  - The **link text**.\n",
    "  - The **URL** (from the `href` attribute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60278af-9613-436f-ab3a-18a02d83db52",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "\n",
    "for a in soup.find_all(\"a\", href=True):\n",
    "    links.append({\n",
    "        \"text\": a.text.strip(),\n",
    "        \"url\": a[\"href\"]\n",
    "    })\n",
    "\n",
    "links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffe6da7-acf0-44d2-9496-7ba0cf3459d5",
   "metadata": {},
   "source": [
    "### 8. Extract Table\n",
    "- Locate the first table on the page (typically the infobox or summary table in Wikipedia articles).\n",
    "- Extract the table structure and its data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d21b84-6209-424e-9e4a-a768166fc121",
   "metadata": {},
   "outputs": [],
   "source": [
    "infobox = soup.find(\"table\", class_=\"infobox\")\n",
    "\n",
    "table_data = []\n",
    "\n",
    "if infobox:\n",
    "    rows = infobox.find_all(\"tr\")\n",
    "    for row in rows:\n",
    "        header = row.find(\"th\")\n",
    "        value = row.find(\"td\")\n",
    "        if header and value:\n",
    "            table_data.append([header.text.strip(), value.text.strip()])\n",
    "else:\n",
    "    print(\"Infobox table not found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64eb542-725b-486a-81bb-e508e54e3190",
   "metadata": {},
   "source": [
    "### 9. Convert Table into a DataFrame\n",
    "- Use `pandas` to convert the table into a DataFrame.\n",
    "- Ensure the table headers and rows are correctly assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63ef32a-8ddf-49a3-bf2c-9285398f0aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(table_data, columns=[\"Attribute\", \"Value\"])\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e70c8ac-6e14-412f-a115-72306a15bf51",
   "metadata": {},
   "source": [
    "### 10. Export the Table\n",
    "- Export the DataFrame as a summary table into a excel file named `samsung_summary_table.xlsx`.\n",
    "- Save the file in the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7878369a-01a6-455c-b143-c52ca29ad40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"samsung_summary_table.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709effa9-3a05-4a09-8520-6c61c1840c89",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "This lab covered web scraping techniques using Python libraries such as requests, BeautifulSoup, and pandas. The extracted data was analyzed, stored in structured formats, and saved for further use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f05e8c9-ed4d-4743-ace5-e8317f2f8949",
   "metadata": {},
   "source": [
    "#### Thank You!\n",
    "\n",
    "Thank you for participating in this lab session! Keep exploring different web scraping techniques and ethical considerations while extracting data from websites."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
